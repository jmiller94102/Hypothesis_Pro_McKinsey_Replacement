"""Test script to demonstrate rich tree generation with research context.

This shows the difference between:
1. Direct tool call (generic output)
2. Tool call with mock research (rich, problem-specific output like the example)
"""

from strategic_consultant_agent.tools.hypothesis_tree import generate_hypothesis_tree
import json

# The problem statement from the example
problem = "Should we scale the deployment of computer vision fall detection in senior living?"

print("=" * 80)
print("TEST 1: Direct Tool Call (NO research context)")
print("=" * 80)
print("\nThis produces GENERIC output because no research context is provided:\n")

tree_generic = generate_hypothesis_tree(
    problem=problem,
    framework="scale_decision",
    use_llm_generation=False  # Force template-based generation
)

# Show just one L2 branch to see the difference
l1_key = list(tree_generic["tree"].keys())[0]
l2_key = list(tree_generic["tree"][l1_key]["L2_branches"].keys())[0]
sample_l2 = tree_generic["tree"][l1_key]["L2_branches"][l2_key]

print(f"Sample L2 Branch: {sample_l2['label']}")
print(f"Question: {sample_l2['question']}")
print(f"\nSample L3 Leaf:")
if sample_l2["L3_leaves"]:
    sample_l3 = sample_l2["L3_leaves"][0]
    print(json.dumps(sample_l3, indent=2))

print("\n" + "=" * 80)
print("TEST 2: Tool Call WITH Mock Research Context")
print("=" * 80)
print("\nThis produces RICH, problem-specific output like the example:\n")

# Mock research context (simulating what the research agents would provide)
mock_market_research = """
**Market Size & Growth:**
- Senior living fall management market: $2.8B (2024), growing 8.5% annually
- 800,000+ assisted living units in US, average fall rate 1.5/resident/year
- Fall-related costs average $35,000/incident (medical + liability)

**Industry Trends:**
- Computer vision adoption in senior living: 12% penetration (2024), up from 3% (2022)
- KLAS 2024 study shows 30-40% reduction in fall-related ER visits for deployed systems
- CMS now incentivizes fall prevention tech in quality metrics (2024)

**Benchmarks:**
- Leading vendors (Teton.ai, SafelyYou, Vayyar) report 25-35% incident reduction
- Typical ROI: 18-24 months
- Staff acceptance rates: 70-80% positive in first 6 months
"""

mock_competitor_research = """
**Key Vendors:**
1. **SafelyYou** - Market leader, 500+ facilities, $50M Series B (2023)
   - Pricing: $150-200/unit/month
   - Strengths: Proven ROI, strong EHR integration
   - Customer feedback: 4.2/5 stars, "transformative but pricey"

2. **Teton.ai** - Fast-growing, 200+ facilities
   - Pricing: $120-150/unit/month
   - Strengths: Best-in-class AI, real-time alerts
   - Case study: Reduced fall-related ER visits by 38% at Spring Hills (200-unit community)

3. **Vayyar** - Privacy-focused radar solution
   - Pricing: $100-130/unit/month
   - Strengths: No cameras (privacy), lower deployment cost
   - Weaknesses: Lower accuracy than vision-based (85% vs 92%)

**Market Dynamics:**
- Installation time: 2-4 weeks per facility
- Support SLAs: 99.5% uptime standard
- Integration challenges: HL7 integration for EHR required
"""

# Generate tree with research context
tree_rich = generate_hypothesis_tree(
    problem=problem,
    framework="scale_decision",
    market_research=mock_market_research,
    competitor_research=mock_competitor_research,
    use_llm_generation=True  # Enable LLM generation (default)
)

# Show the same L2 branch to compare
l1_key_rich = list(tree_rich["tree"].keys())[0]
l2_key_rich = list(tree_rich["tree"][l1_key_rich]["L2_branches"].keys())[0]
sample_l2_rich = tree_rich["tree"][l1_key_rich]["L2_branches"][l2_key_rich]

print(f"Sample L2 Branch: {sample_l2_rich['label']}")
print(f"Question: {sample_l2_rich['question']}")
print(f"\nSample L3 Leaf:")
if sample_l2_rich["L3_leaves"]:
    sample_l3_rich = sample_l2_rich["L3_leaves"][0]
    print(json.dumps(sample_l3_rich, indent=2))

print("\n" + "=" * 80)
print("KEY DIFFERENCES:")
print("=" * 80)
print("\nGeneric (no research):")
print("- Uses template L2/L3 structure")
print("- Generic questions and targets")
print("- No specific benchmarks or vendors mentioned")
print("\nRich (with research):")
print("- Problem-specific L2 branches generated by LLM")
print("- L3 leaves reference specific benchmarks (e.g., 'KLAS 2024 study')")
print("- Mentions specific vendors (e.g., 'Teton.ai case study')")
print("- Targets based on real industry data (e.g., '30-40% reduction')")

print("\n" + "=" * 80)
print("HOW TO GET RICH OUTPUT IN YOUR AGENT:")
print("=" * 80)
print("""
1. **Use the full multi-agent workflow** (not direct tool calls):
   - Research agents gather market and competitor data
   - Hypothesis generator receives that research as context
   - Generator calls generate_hypothesis_tree WITH research params

2. **Ensure research agents run first**:
   - ParallelAgent runs market_researcher and competitor_researcher
   - They use google_search to gather real data
   - Results stored in session state as {market_research} and {competitor_research}

3. **Hypothesis generator references research in its prompt**:
   - System prompt includes: {market_research} and {competitor_research}
   - Agent passes these to generate_hypothesis_tree tool
   - Tool enables use_llm_generation=True (default)

4. **For testing without API costs**:
   - Use mock research data like shown above
   - Or modify agents to use pre-loaded research snippets
   - This gives you the rich output without search API calls
""")

print("\n" + "=" * 80)
print("SAVING RICH OUTPUT TO FILE:")
print("=" * 80)

output_file = "test_output_rich_tree.json"
with open(output_file, "w") as f:
    json.dump(tree_rich, f, indent=2)

print(f"\n✓ Saved rich tree output to: {output_file}")
print("✓ Compare this to docs/project-requirements/examples/mece_tree_fall_detection.json")
print("\nThe structure and specificity should be similar!")
